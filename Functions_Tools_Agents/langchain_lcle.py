# -*- coding: utf-8 -*-
"""LangChain_LCLE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1olwCTKDdoif5dBl5Vl7p4JktCTVknkPY
"""

from google.colab import drive
drive.mount('/content/drive/', force_remount=True)

openai_api_key =

!pip install langchain-community langchain-core --quiet

from langchain.prompts import ChatPromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.schema.output_parser import StrOutputParser # take the chat message and convert into string

"""## Simple Chain"""

prompt = ChatPromptTemplate.from_template(
    "tell me a short joke about {topic}"
)
model = ChatOpenAI(openai_api_key= openai_api_key)
output_parser = StrOutputParser()

chain = prompt | model | output_parser   # we need to conect them with pipe syntax

chain.invoke({"topic": "bears"}) # call them using invoke method

"""## More complex chain

- And Runnable Map to supply user-provided inputs to the prompt.
"""

! pip install docarray --quiet
! pip install tiktoken --quiet

from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import DocArrayInMemorySearch

vectorstore = DocArrayInMemorySearch.from_texts(
    ["harrison worked at kensho", "bears like to eat honey"],
    embedding = OpenAIEmbeddings(openai_api_key= openai_api_key)
)
retriever = vectorstore.as_retriever()

retriever.get_relevant_documents("where did harrison work?")

retriever.get_relevant_documents("what do bears like to eat")

template = """Answer the question based only on the following context:
{context}

Question: {question}
"""
prompt = ChatPromptTemplate.from_template(template)  # question --> user question, template --> context

from langchain.schema.runnable import RunnableMap

chain = RunnableMap({    # this runnable map has two components
    "context": lambda x: retriever.get_relevant_documents(x["question"]),
    "question": lambda x: x["question"]
}) | prompt | model | output_parser

chain.invoke({"question": "where did harrison work?"})

inputs = RunnableMap({
    "context": lambda x: retriever.get_relevant_documents(x["question"]),
    "question": lambda x: x["question"]
})

inputs.invoke({"question": "where did harrison work?"})

"""## Bind

- Bind the parameters
- and OpenAI Functions
"""

functions = [
    {
      "name": "weather_search",
      "description": "Search for weather given an airport code",
      "parameters": {
        "type": "object",
        "properties": {
          "airport_code": {
            "type": "string",
            "description": "The airport code to get the weather for"
          },
        },
        "required": ["airport_code"]
      }
    }
  ]

import os
os.environ["OPENAI_API_KEY"] =

prompt = ChatPromptTemplate.from_messages(
    [
        ("human", "{input}")
    ]
)
model = ChatOpenAI(temperature=0).bind(functions=functions)

runnable = prompt | model

runnable.invoke({"input": "what is the weather in sf"})

functions = [
    {
      "name": "weather_search",
      "description": "Search for weather given an airport code",
      "parameters": {
        "type": "object",
        "properties": {
          "airport_code": {
            "type": "string",
            "description": "The airport code to get the weather for"
          },
        },
        "required": ["airport_code"]
      }
    },
        {
      "name": "sports_search",
      "description": "Search for news of recent sport events",
      "parameters": {
        "type": "object",
        "properties": {
          "team_name": {
            "type": "string",
            "description": "The sports team to search for"
          },
        },
        "required": ["team_name"]
      }
    }
  ]

model = model.bind(functions = functions)

runnable = prompt | model

runnable.invoke({"input": "how did the alex do yesterday?"})

"""## Fallbacks"""

from langchain.llms import OpenAI  # the older version
import json

simple_model = OpenAI(
    temperature= 0,
    max_tokens= 1000,
    model="gpt-3.5-turbo-instruct"  # early version of openai model
)
simple_chain = simple_model | json.loads  # call model --> output results

challenge = "write three poems in a json blob, where each poem is a json blob of a title, author, and first line"

simple_model.invoke(challenge)

simple_chain.invoke(challenge)  # expected to be fail

model = ChatOpenAI(temperature=0) # the new model of openai are good at outputing the json
chain = model | StrOutputParser() | json.loads # model --> output parser taking the chat message and converting to string --> converting the json.load

chain.invoke(challenge)

final_chain = simple_chain.with_fallbacks([chain])

final_chain.invoke(challenge)

"""## Interface"""

prompt = ChatPromptTemplate.from_template(
    "Tell me a short joke about {topic}"
)
model = ChatOpenAI()
output_parser = StrOutputParser()

chain = prompt | model | output_parser

chain.invoke({"topic": "bears"})

"""- invoke: accept input -retuen output
- batch : accept a list of inputs & return a list of outputs
- stream : accept an input and retun a generator
"""

chain.batch([{"topic": "bears"}, {"topic": "frogs"}])

for t in chain.stream({"topic": "bears"}):
    print(t)

response = await chain.ainvoke({"topic": "bears"})
response