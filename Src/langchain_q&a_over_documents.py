# -*- coding: utf-8 -*-
"""LangChain: Q&A over Documents.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q0i5aVTJ7PbDpOtgTc_EUx1tn2m_61RY

LangChain: Q&A over DocumentsÂ¶

An example might be a tool that would allow you to query a product catalog for items of interest.
"""

from google.colab import drive
drive.mount('/content/drive/', force_remount=True)

!pip install --upgrade langchain --quiet
!pip install langchain-community langchain-core --quiet
!pip install docarray --quiet
!pip install tiktoken --quiet

import os
import openai
import warnings
warnings.filterwarnings('ignore')

openai_api_key = "..."

from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import CSVLoader
from langchain.vectorstores import DocArrayInMemorySearch
from IPython.display import display, Markdown
from langchain.llms import OpenAI

from langchain.embeddings import OpenAIEmbeddings

from langchain.indexes import VectorstoreIndexCreator # import index, the vector store index creator--> help to create a vectore store very easy

# account for deprecation of LLM model
import datetime
# Get the current date
current_date = datetime.datetime.now().date()

# Define the date after which the model should be set to "gpt-3.5-turbo"
target_date = datetime.date(2024, 6, 12)

# Set the model variable based on the current date
if current_date > target_date:
    llm_model = "gpt-3.5-turbo"
else:
    llm_model = "gpt-3.5-turbo-0301"

csv_file_path = '/content/drive/MyDrive/Project_Experiments/DeepLearning_LangChain/QnA_data/OutdoorClothingCatalog_1000.csv'

# Initialize CSVLoader with the verified file path
loader = CSVLoader(file_path = csv_file_path)

# Load and print a count of documents to verify the data is read correctly
documents = loader.load()
print(f"Loaded {len(documents)} documents.")

# Initialize your embedding model
embedding_model = OpenAIEmbeddings(openai_api_key = openai_api_key)

# Now create the index with the embedding model
index = VectorstoreIndexCreator(
    vectorstore_cls = DocArrayInMemorySearch,
    embedding=embedding_model
).from_loaders([loader])

query ="Please list all your shirts with sun protection \
in a table in markdown and summarize each one."

"""Note:

    The notebook uses langchain==0.0.179 and openai==0.27.7
    For these library versions, VectorstoreIndexCreator uses text-davinci-003 as the base model, which has been deprecated since 1 January 2024.
    The replacement model, gpt-3.5-turbo-instruct will be used instead for the query.
    The response format might be different than the video because of this replacement model.


"""

llm_replacement_model = OpenAI(temperature=0,
                               model='gpt-3.5-turbo-instruct',openai_api_key = openai_api_key)

response = index.query(query,
                       llm = llm_replacement_model)

display(Markdown(response))

"""# steo by step"""

# create document loader, loading from that CSV with all the descriptions we wanna get QnA over
from langchain.document_loaders import CSVLoader
loader = CSVLoader(file_path = csv_file_path)

docs = loader.load()

docs[0]

"""# After loading we need to chunk, but because the text here is small, we can continue with embedding"""

embeddings = OpenAIEmbeddings(openai_api_key = openai_api_key) # use openai embeddings class

embed = embeddings.embed_query("Hi my name is Tannaz")

print(len(embed)) # thousand different elements

print(embed[:5]) # each of these elemnts has a different numerical value, combined create the overall piece of text

"""# we want to embed for all the piece of text we have just loaded and store them in the vactore store"""

db = DocArrayInMemorySearch.from_documents(docs,embeddings) # doc loaded from my dataset/ emebeddings from library (convert documents into vector representations)

query = "Please suggest a shirt with sunblocking"

docs = db.similarity_search(query) # use similar serach method

len(docs)

docs[0]

"""# how to use this over QnA"""

retriever = db.as_retriever() # convert query into embedding and return the most similar one

llm = ChatOpenAI(temperature = 0.0, model=llm_model,openai_api_key = openai_api_key)

len(docs)

#qdocs = "".join([docs[i].page_content for i in range(len(docs))])
qdocs = "".join([docs[i].page_content for i in range(4)])  # takes Document objects, extracts their .page_content strings, and joins them into one block of text, separated by newlines.

len(qdocs)#[0]

"""## Sends a prompt to your LLM that consists of the 4 retrieved document texts plus the question at the end. The model will read that combined context and generate an answer."""

response = llm.call_as_llm(f"{qdocs} Question: Please list all your \
shirts with sun protection in a table in markdown and summarize each one.")

display(Markdown(response))

qa_mapreduce = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="map_reduce",
    retriever=retriever,
    verbose=True
)
response = qa_mapreduce.run(query)
display(Markdown(response))

retriever = db.as_retriever(search_kwargs={"k": 4})

qa_stuff = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",  # concatenates (stuffs) all retrieved docs into the prompt.
    retriever=retriever,
    verbose=True
)

query =  "Please list all your shirts with sun protection in a table  \
          in markdown and summarize each one."

response = qa_stuff.run(query)

display(Markdown(response))

response = index.query(query, llm=llm)

index = VectorstoreIndexCreator(
    vectorstore_cls=DocArrayInMemorySearch,
    embedding=embeddings,
).from_loaders([loader])