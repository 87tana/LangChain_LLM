# -*- coding: utf-8 -*-
"""VectoreStores_Embeddings.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IuhsTbYr09aBJsTNwrL7iFPuOa8ykkbR

## Vectorstores and Embeddings

- Recall the overall workflow for retrieval augmented generation (RAG):
"""

from google.colab import drive
drive.mount('/content/drive/', force_remount=True)

import os
import openai
import sys
sys.path.append('../..')


openai_api_key =

!pip install langchain-community langchain-core --quiet

! pip install pypdf --quiet

! pip install tiktoken --quiet
! pip install chromadb  -quiet

from langchain.document_loaders import PyPDFLoader   # we need to install --> pypdf

# Load PDF
loaders = [
    # Duplicate documents on purpose - messy data
    PyPDFLoader("/content/drive/MyDrive/Project_Experiments/DeepLearning_LangChain/docs/cs229_lectures/MachineLearning-Lecture01.pdf"),
    PyPDFLoader("/content/drive/MyDrive/Project_Experiments/DeepLearning_LangChain/docs/cs229_lectures/MachineLearning-Lecture01.pdf"), # by purpose for simulating some dirty data.
    PyPDFLoader("/content/drive/MyDrive/Project_Experiments/DeepLearning_LangChain/docs/cs229_lectures/MachineLearning-Lecture02.pdf"),
    PyPDFLoader("/content/drive/MyDrive/Project_Experiments/DeepLearning_LangChain/docs/cs229_lectures/MachineLearning-Lecture03.pdf"),
]
docs = []
for loader in loaders:
    docs.extend(loader.load())

# Split
from langchain.text_splitter import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 1500,
    chunk_overlap = 150
)

splits = text_splitter.split_documents(docs)

len(splits)

"""## Embeddings

- Let's take our splits and embed them.

"""

from langchain.embeddings.openai import OpenAIEmbeddings
embedding = OpenAIEmbeddings(openai_api_key= openai_api_key)

sentence1 = "i like dogs"
sentence2 = "i like canines"
sentence3 = "the weather is ugly outside"

embedding1 = embedding.embed_query(sentence1)  # we need tiktoken installation here
embedding2 = embedding.embed_query(sentence2)
embedding3 = embedding.embed_query(sentence3)

import numpy as np

np.dot(embedding1, embedding2)

np.dot(embedding1, embedding3)

np.dot(embedding2, embedding3)

"""## Vectorstores"""

from langchain.vectorstores import Chroma  # install Chroma --> lightweight and in memory

persist_directory = ("/content/drive/MyDrive/Project_Experiments/DeepLearning_LangChain/docs/chroma/")

!rm -rf ./docs/chroma  # remove old database files if any

vectordb = Chroma.from_documents(
    documents =splits,
    embedding =embedding,
    persist_directory =persist_directory  # it's a chroma specific keyword arguments that allows us to save the directory to disk.
)

print(vectordb._collection.count())

"""## Similarity Search"""

question = "is there an email i can ask for help"

docs = vectordb.similarity_search(question,k=3)

len(docs)

docs[0].page_content

vectordb.persist()  #  saves the in-memory database to disk at your persist_directory.

"""## Failure modes

- This seems great, and basic similarity search will get you 80% of the way there very easily.But there are some failure modes that can creep up.

- Here are some edge cases that can arise - we'll fix them in the next class.
"""

question = "what did they say about matlab?"

docs = vectordb.similarity_search(question,k=5)

docs[0]

docs[1]

"""- we need to know how to retriev relevant and distinct chunk at the same time"""

question = "what did they say about regression in the third lecture?"

docs = vectordb.similarity_search(question,k=5)

for doc in docs:
    print(doc.metadata)

"""- the reason that we get the answers from other lecture is, we are looking for semantic lookup basded on embeddings (regression) that's why we see other things --> it's kind of failure."""

print(docs[3].page_content)